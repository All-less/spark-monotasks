diff --git a/experiment_scripts/run_bdb_experiments.py b/experiment_scripts/run_bdb_experiments.py
index 6af91732e..cd25b2286 100644
--- a/experiment_scripts/run_bdb_experiments.py
+++ b/experiment_scripts/run_bdb_experiments.py
@@ -226,6 +226,7 @@ def execute_query(aws_key_id, aws_key, args, query, branch, is_first_branch):
   print "Retrieving logs"
   parameters = [query, branch]
   log_dir = utils.copy_all_logs(parameters, utils.get_workers())
+  utils.copy_all_traces(log_dir, driver_addr, utils.get_workers())
   log_files = path.join(log_dir, "*")
 
   # Move the logs into a new directory: output_dir/query/branch/
diff --git a/experiment_scripts/utils.py b/experiment_scripts/utils.py
index d84220ec1..76e854034 100644
--- a/experiment_scripts/utils.py
+++ b/experiment_scripts/utils.py
@@ -99,6 +99,11 @@ def copy_all_logs(stringified_parameters, slaves):
   print "Finished copying results to {}".format(log_directory_name)
   return log_directory_name
 
+def copy_all_traces(log_dir, master, slaves):
+  for slave in slaves:
+    scp_from(slave, "/root/call_stack.trace", "{}/slave_{}.trace".format(log_dir, slave))
+  scp_from(master, "/root/call_stack.trace", "{}/master_{}.trace".format(log_dir, master))
+
 def copy_and_zip_all_logs(stringified_parameters, slaves):
   """ Packages up all of the logs from running an experiment.
 
diff --git a/sbin/spark-daemon.sh b/sbin/spark-daemon.sh
index 5e812a1d9..6e44bd264 100755
--- a/sbin/spark-daemon.sh
+++ b/sbin/spark-daemon.sh
@@ -146,6 +146,8 @@ case $option in
     if [ $option == spark-submit ]; then
       source "$SPARK_HOME"/bin/utils.sh
       gatherSparkSubmitOpts "$@"
+      echo "SUBMISSION_OTPS: ${SUBMISSION_OPTS[@]}"
+      echo "APPLICATION_OPTS: ${APPLICATION_OPTS[@]}"
       nohup nice -n $SPARK_NICENESS "$SPARK_PREFIX"/bin/spark-submit --class $command \
         "${SUBMISSION_OPTS[@]}" spark-internal "${APPLICATION_OPTS[@]}" >> "$log" 2>&1 < /dev/null &
     else
